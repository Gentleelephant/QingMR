{
	"Cluster properties" : "集群属性",
	"name" : "名称",
	"The name of the service" : "服务名称",
	"description" : "描述",
	"The description of the service" : "服务描述",
	"VxNet" : "私有网络",
	"Choose a vxnet to join" : "选择应用运行的私有网络",
	"Choose another application service to use, all the nodes in this external service will be added to hosts of current application service" : "选择需要依赖的其他应用服务，该服务里的所有节点将被添加到当前应用服务所有节点的hosts文件里",
	"CPUs of each node" : "每个节点的CPU数量",
	"Memory" : "内存",
	"memory of each node (in MB)" : "每个节点的内存大小（单位MB）",
	"count" : "节点数量",
	"Number of nodes for the cluster to create" : "待创建集群里该类节点数量",
	"instance class" : "节点类型",
	"The instance type for the cluster to run, such as high performance, high performance plus" : "节点分为性能型、超高性能型",
	"volume class" : "数据盘类型",
	"The volume type for each instance, such as high performance, high performance plus" : "数据盘分为性能型、超高性能型",
	"volume size" : "节点容量",
	"The volume size for each instance" : "节点总存储空间大小", 
	"HDFS master" : "HDFS主节点",
	"HDFS master properties" : "HDFS主节点(Name Node)属性",
	"slave" : "从节点",
	"Slave properties" : "从节点（包含Spark worker, Yarn NodeManager和HDFS Data Node）属性",
	"BigData client" : "BigData client节点",
	"BigData client properties" : "BigData client节点属性",
	"Service properties" : "服务属性",
	"The number of server threads for the data node" : "Data node节点服务线程数",
	"The number of server threads for the name node" : "Name node节点服务线程数",
	"The replication factor in HDFS" : "HDFS副本数",
	"It controls the number of minutes after which a trash checkpoint directory is deleted" : "控制Trash检查点目录过多少分钟后被删除",
	"Running" : "运行中的YARN应用",
	"running_0" : "小于60分钟",
	"running_60" : "60~300分钟",
	"running_300" : "300~1440分钟",
	"running_1440" : "1440分钟以上",
	"Applications" : "YARN应用",
	"SparkWorkers" : "Workers(Spark Standalone模式)",
	"SparkApps" : "Applications(Spark Standalone模式)",
	"WorkersTotal" : "Total",
	"WorkersAlive" : "Alive",
	"FilesTotal" : "Total",
	"FilesCreated" : "Created",
	"FilesAppended" : "Appended",
	"FilesRenamed" : "Renamed",
	"FilesDeleted" : "Deleted",
	"RemainingGB" : "Remaining",
	"LiveNodes" : "Live",
	"DeadNodes" : "Dead",
	"DecomLiveNodes" : "DecomLive",
	"DecomDeadNodes" : "DecomDead",
	"DecommissioningNodes" : "Decommissioning",
	"MemUsedMB" : "Used",
	"MemFreeMB" : "Free",
	"BlocksRead" : "Read",
	"BlocksWritten" : "Written",
	"NMMemory" : "NodeManager内存",
	"Compute" : "计算(Spark Standalone)",
	"WorkerMemory" : "内存(Spark Standalone)",
	"Storage" : "存储",
	"Blocks" : "DFS块",
	"Gc" : "垃圾回收",
	"DFS Files" : "DFS文件",
	"DFS Percentage" : "DFS空间占比",
	"DFS Capacity" : "DFS容量",
	"Hadoop proxy user" : "Hadoop代理用户",
	"Hosts the proxyuser can represent" : "Hadoop代理用户能代理哪些hosts",
	"Groups in hosts the proxyuser can represent" : "Hadoop代理用户能代理指定host中的哪些groups",
	"Enable periodic cleanup of worker/application directories. Only the directories of stopped applications are cleaned up" : "定期清理应用work目录，运行中的application不会被清理。",
	"Controls the interval, in seconds, at which the worker cleans up old application work dirs on the local machine, default to 28800 seconds(8 hours)" : "清理应用work目录的时间间隔，以秒为单位，默认为28800秒（8小时）",
	"The number of seconds to retain application work directories on each worker, default to 86400 seconds(24 hours)" : "保留worker上应用work目录的时间，以秒为单位，默认为86400秒(24 小时)",
	"Memory(in MB) allocated to spark master daemon(standalone mode). The upper limit is total memory - 1024" : "Spark master(Standalone模式)进程占用内存(MB)。该值上限定为总内存-1024",
	"Memory(in MB) allocated to spark worker daemon(standalone mode). The upper limit is total memory - 1024" : "Spark worker(Standalone模式)进程占用内存(MB)。该值上限定为总内存-1024",
	"The maximum amount of heap(in MB) to use by resource manager. It will be reset to current available free memory if 1000 is specified" : "Resource manager可用最大堆内存大小，如果指定1000，则Resource manager将可利用当前所有空闲内存",
	"The maximum amount of heap to use by datanode in MB, Default is 1000. The upper limit is total memory - 1024" : "Datanode daemon进程可用的最大heap大小，以MB为单位，默认值为1000. 该值上限为总内存-1024",
	"Specify the python version used by a python spark job, current supported python versions are 2.7.13 and 3.6.1. Anaconda distribution data science packages numpy, scikit-learn, scipy, Pandas, NLTK and Matplotlib are also included." : "指定Python Spark程序所用的Python版本，目前支持Python 2.7.13和3.6.1。两个Python版本对应的Anaconda发行版的数据科学包numpy, scikit-learn, scipy, Pandas, NLTK and Matplotlib也可以被调用。"
}
