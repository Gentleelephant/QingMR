#! /usr/bin/env bash
export SPARK_LOG_DIR=/bigdata1/spark/logs
export SPARK_WORKER_DIR=/bigdata1/spark/work
export SPARK_WORKER_OPTS="-Dspark.worker.cleanup.enabled={{getv "/env/spark.worker.cleanup.enabled"}} -Dspark.worker.cleanup.interval={{getv "/env/spark.worker.cleanup.interval"}} -Dspark.worker.cleanup.appDataTtl={{getv "/env/spark.worker.cleanup.appDataTtl"}}"
export SPARK_PID_DIR=/bigdata1/spark/pids
export SPARK_LOCAL_DIRS=/bigdata1/spark
export PYSPARK_PYTHON=/opt/{{getv "/env/PYSPARK_PYTHON"}}/bin/python
export HADOOP_HOME=/opt/hadoop/etc/hadoop
export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop
{{range $dir := lsdir "/hosts/spark-master/"}}{{$ip := printf "/hosts/spark-master/%s/ip" $dir}}
export SPARK_MASTER_IP={{getv $ip}}{{end}}
{{$master_daemon_memory := getv "/env/spark.master.SPARK_DAEMON_MEMORY"}}{{$master_memory := div (div (getv "/host/memory") 1024) 2}}{{if and (ne $master_daemon_memory 0) (lt $master_daemon_memory $master_memory)}}
export SPARK_DAEMON_MEMORY={{$master_daemon_memory}}g
{{else}}
export SPARK_DAEMON_MEMORY={{$master_memory}}g
{{end}}
